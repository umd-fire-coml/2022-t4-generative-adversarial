{"cells":[{"cell_type":"markdown","metadata":{"id":"2ZJupI5dJFwE"},"source":["# Model Tester\n","Use this notebook to test/demonstrate model results.\n","Since the model generates new audio on each run, it can't exactly be \"tested\". However, it can be qualitatively (subjectively) evaluated by the human tester.\n","\n","See: https://stackoverflow.com/a/69897034\n","\n","> There is no testing phase in GANS as we normally have in other neural networks like CNN etc. GAN generator models are evaluated based on the quality of the images generated, often in the context of the target problem domain.\n","\n","This notebook mounts your Google Drive so you can point it to a custom dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Setup\n","Installs dependencies, downloads pretrained models and weights"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nfnlPfOcKZqK"},"outputs":[],"source":["%cd /content\n","!git clone https://github.com/marcoppasini/musika\n","%cd musika\n","!pip install -r requirements.txt\n","!pip install scikit-learn\n","!pip install gensim\n","!pip install nltk\n","!pip install transformers\n","!pip install --upgrade --no-cache-dir gdown\n","!apt install ffmpeg unzip\n","\n","import sys\n","import os\n","import gdown\n","import subprocess\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","weights_link = \"https://drive.google.com/uc?id=1RDNjpcAH10JQ87HAvYtYCaIfnO_ZLf8T\"\n","if not os.path.exists(\"./weights.zip\"):\n","  gdown.download(weights_link, \"./weights.zip\")\n","if not os.path.exists(\"./weights\"):\n","  subprocess.check_output([\"unzip\", \"./weights.zip\"])\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",)\n","model = AutoModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True)\n","\n","def get_embeddings(text,token_length):\n","  tokens=tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n","  output=model(torch.tensor(tokens.input_ids).unsqueeze(0),\n","               attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n","  return torch.mean(output,axis=1).detach().numpy()\n","  \n","genre_list = [genre.replace(\"/\", \"\") for genre in os.listdir(\"./weights\")]\n","genre_embeddings = [get_embeddings(genre, 20) for genre in genre_list]\n","\n","def get_closest(query, token_length=20):\n","    query_embeddings = get_embeddings(query, token_length)\n","    sims = [cosine_similarity(embed, query_embeddings)[0][0] for embed in genre_embeddings]\n","    closest = max(zip(sims, genre_list))[1]\n","    return closest"]},{"cell_type":"markdown","metadata":{},"source":["### Generate Audio\n","This cell generates audio based on the genre from user input. The user may subjectively evaluate audio on whatever criteria they deem fit for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuXkzVX7KSG7"},"outputs":[],"source":["from IPython.display import Audio, display\n","from IPython.utils import io\n","import subprocess\n","import glob\n","\n","genre = \"pop\" #@param {type:\"string\"}\n","coerced_genre = get_closest(genre)\n","\n","output_dir = \"/content/musika/output\"\n","already_output_files = glob.glob(output_dir + \"/*\")\n","for f in already_output_files:\n","  os.remove(f)\n","\n","weights_path = f\"./weights/{coerced_genre}\"\n","\n","!python ./musika_generate.py --load_path $weights_path --num_samples 1 --seconds 30 --save_path ./output\n","\n","output_file = os.path.join(output_dir, os.listdir(output_dir)[0])\n","display(Audio(output_file))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO0gcf7L1V3AltJFLp0iqo4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}
